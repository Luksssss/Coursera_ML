{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import model_selection, datasets, metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./kaggle/orange_small_churn_train_data.csv', sep = ',', header = 0, index_col=0)\n",
    "test_data = pd.read_csv('./kaggle/orange_small_churn_test_data.csv', sep = ',', header = 0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18298, 230)\n",
      "(18298, 1)\n",
      "(10000, 230)\n"
     ]
    }
   ],
   "source": [
    "# т.к. в последней строке тренировочных данных ответа нету, то удаляем её\n",
    "train_data = data.iloc[:-1,:-1]\n",
    "train_labels = data.iloc[:-1,-1:]\n",
    "\n",
    "print train_data.shape\n",
    "print train_labels.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отделяем числовые и категориальные признаки и удаляем полностью пустые признаки\n",
    "data_numb = train_data.iloc[:,0:190].dropna(axis=1, how='all')\n",
    "data_categ = train_data.iloc[:,190:].dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = data_numb.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заменим NONE на среднее значение колонки\n",
    "# Посчитаем средние по колонкам\n",
    "numeric_means = data_numb.mean(axis=0, skipna=True)\n",
    "\n",
    "# Заполним пропущенные численные значения средними\n",
    "data_numb = data_numb.fillna(numeric_means, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradientBoostingClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b5a6957fc7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# попробуем обучить только вещественные признаки чтобы понять какие признакие самые важные\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m select = RFE(GradientBoostingClassifier(n_estimators = 110, random_state=0, learning_rate=0.1, max_depth=1),\n\u001b[0m\u001b[1;32m      3\u001b[0m             n_features_to_select=10)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradientBoostingClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# попробуем обучить только вещественные признаки чтобы понять какие признакие самые важные\n",
    "select = RFE(GradientBoostingClassifier(n_estimators = 110, random_state=0, learning_rate=0.1, max_depth=1),\n",
    "            n_features_to_select=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 5s, sys: 120 ms, total: 4min 5s\n",
      "Wall time: 4min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "  n_features_to_select=10, step=1, verbose=0)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "select.fit(data_numb, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_top10=[columns_name[ind] for ind, col in enumerate(select.support_) if col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numb = data_numb[columns_top10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВЕЩЕСТВЕННЫЕ признаки\n",
    "# NOTE: сделаем скелинг численных признаков с D=1 (подробнее см. неделю 1)\n",
    "# NOTE2: такая запись по созданию нового DataFrame нужна из-за тоог что при fit_transform слетают индексы и потом\n",
    "# при контатенации у нас получается каша.\n",
    "scaler = StandardScaler()\n",
    "data_numb=pd.DataFrame(scaler.fit_transform(data_numb.values), index=data_numb.index, columns=columns_top10)\n",
    "\n",
    "# меняем пустые на 0 (0 это среднее так как признаки масштабированы)\n",
    "# data_numb.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# КАТЕГОРИАЛЬНЫЕ признаки\n",
    "# заменим пустые значения на NA (будет как доп.признак)\n",
    "data_categ = data_categ.fillna('NA').applymap(lambda s: str(s))\n",
    "\n",
    "# удалим те колонки где больше 20 категорий и меньше 2 (эти колонки не информативны)\n",
    "name_del = [name for name, var in data_categ.iteritems() if var.value_counts(dropna=True).shape[0] > 20 or var.value_counts(dropna=True).shape[0] < 2]\n",
    "data_categ = data_categ.drop(labels=name_del, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВАРИАНТ 1 кодируем категориальные признаки\n",
    "# data_dummies = pd.get_dummies(data_categ)\n",
    "\n",
    "\n",
    "# ВАРИАНТ 2 one-hote-encoder\n",
    "encoder = DV(sparse = False)\n",
    "data_dummies = encoder.fit_transform(data_categ.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18298, 101)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# при ВАРИАНТЕ 1\n",
    "# объединяем числовые признаки и закодированные категориальные\n",
    "# train_data = pd.concat([data_numb, data_dummies], axis=1)\n",
    "# train_data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# при ВАРИАНТЕ 2\n",
    "train_data = np.hstack((data_numb.values, data_dummies))\n",
    "# train_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18298, 10)\n",
      "(18298, 20)\n",
      "(18298, 111)\n",
      "(18298, 1)\n"
     ]
    }
   ],
   "source": [
    "print data_numb.shape\n",
    "print data_categ.shape\n",
    "\n",
    "print train_data.shape\n",
    "print train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_depth - максимальная глубина\n",
    "# learning_rate - насколько сильно каждое дерево будет пытаться исправить ошибки предыдущих деревьев.\n",
    "parameters_grid = {\n",
    "    'n_estimators' : range(90, 115, 5),\n",
    "    'max_depth': range(1, 8, 1)\n",
    "}\n",
    "\n",
    "# Будем использовать метод стратификации который делит соотношение классов в обучающей выборке на равное количество\n",
    "skf = model_selection.StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "classifier = RandomForestClassifier(random_state=0, class)\n",
    "grid_rfc = model_selection.GridSearchCV(classifier, parameters_grid, scoring = 'roc_auc', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# max_depth - максимальная глубина\n",
    "# learning_rate - насколько сильно каждое дерево будет пытаться исправить ошибки предыдущих деревьев.\n",
    "parameters_grid = {\n",
    "    'n_estimators' : range(80, 120, 5),\n",
    "    'learning_rate' : [0.05, 0.1, 0.07, 0.08, 0.09],\n",
    "    'max_depth': range(1, 5, 1)\n",
    "}\n",
    "\n",
    "# Будем использовать метод стратификации который делит соотношение классов в обучающей выборке на равное количество\n",
    "skf = model_selection.StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "classifier = GradientBoostingClassifier(random_state=0)\n",
    "grid_rfc = model_selection.GridSearchCV(classifier, parameters_grid, scoring = 'roc_auc', cv = skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 9min 17s, sys: 8.38 s, total: 1h 9min 25s\n",
      "Wall time: 1h 3min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [90, 95, 100, 105, 110], 'learning_rate': [0.05, 0.1, 0.07, 0.08, 0.09], 'max_depth': [1, 2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_rfc.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=110,\n",
       "              n_iter_no_change=None, presort='auto', random_state=0,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7251141392953343\n",
      "{'n_estimators': 110, 'learning_rate': 0.1, 'max_depth': 1}\n"
     ]
    }
   ],
   "source": [
    "print grid_rfc.best_score_\n",
    "print grid_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=110,\n",
       "              n_iter_no_change=None, presort='auto', random_state=0,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# строим модель с оптимальными параметрами которые удалось подобрать\n",
    "clf = GradientBoostingClassifier(n_estimators = 110, random_state=0, learning_rate=0.1, max_depth=1)\n",
    "clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем метрики на тренировочном наборе\n",
    "actual_labels = clf.predict(train_data)\n",
    "# actual_labels_proba = clf.predict_proba(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_ROC = 0.5021491001564492\n",
      "accuracy = 0.9250191277735271\n",
      "precision = 0.8571428571428571\n",
      "recall = 0.004357298474945534\n",
      "f1 = 0.008670520231213874\n"
     ]
    }
   ],
   "source": [
    "print \"AUC_ROC =\", metrics.roc_auc_score(train_labels, actual_labels)\n",
    "print \"accuracy =\", clf.score(train_data, train_labels)\n",
    "print \"precision =\", metrics.precision_score(train_labels, actual_labels)\n",
    "print \"recall =\", metrics.recall_score(train_labels, actual_labels)\n",
    "print \"f1 =\", metrics.f1_score(train_labels, actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC_ROC = 0.5021491001564492\n",
    "# accuracy = 0.9250191277735271\n",
    "# precision = 0.8571428571428571\n",
    "# recall = 0.004357298474945534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.05567181, 0.05632596, 0.05569146, 0.0143665 ,\n",
       "       0.0820897 , 0.30228797, 0.00563952, 0.03133637, 0.11886813,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.0541655 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00984846, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00289404,\n",
       "       0.01003713, 0.00858656, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.1628948 , 0.        ,\n",
       "       0.02590311, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00339298, 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важность признаков\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовый набор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 230)\n"
     ]
    }
   ],
   "source": [
    "# Предобрабатываем тестовый набор\n",
    "print test_data.shape\n",
    "\n",
    "# отделяем числовые и категориальные признаки и удаляем полностью пустые признаки\n",
    "# data_numb_test = test_data.iloc[:,0:190].dropna(axis=1, how='all')\n",
    "data_numb_test = test_data[columns_top10]\n",
    "data_categ_test = test_data.iloc[:,190:]\n",
    "\n",
    "# data_dummies = pd.get_dummies(data_categ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВЕЩЕСТВЕННЫЕ признаки\n",
    "numeric_means_test = data_numb_test.mean(axis=0, skipna=True)\n",
    "# Заполним пропущенные численные значения средними\n",
    "data_numb_test = data_numb_test.fillna(numeric_means_test, axis=0)\n",
    "\n",
    "data_numb_test =pd.DataFrame(scaler.transform(data_numb_test.values), index=data_numb_test.index, columns=columns_top10)\n",
    "\n",
    "# data_numb_test.fillna(0, inplace=True)\n",
    "\n",
    "# КАТЕГОРИАЛЬНЫЕ ПРИЗНАКИ\n",
    "# берем только колонки используемые в обучении\n",
    "data_categ_test = data_categ_test[data_categ.columns]\n",
    "data_categ_test = data_categ_test.fillna('NA').applymap(lambda s: str(s))\n",
    "# удалим те колонки где больше 20 категорий и меньше 2 (эти колонки не информативны)\n",
    "# name_del = [name for name, var in data_categ_test.iteritems() if var.value_counts(dropna=True).shape[0] > 20 or var.value_counts(dropna=True).shape[0] < 2]\n",
    "# data_categ_test = data_categ_test.drop(labels=name_del, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "(10000, 20)\n"
     ]
    }
   ],
   "source": [
    "print data_numb_test.shape\n",
    "print data_categ_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кодируем категориальные признаки\n",
    "# data_dummies_test = pd.get_dummies(data_categ_test)\n",
    "\n",
    "data_dummies_test = encoder.transform(data_categ_test.T.to_dict().values())\n",
    "# data_dummies_test = np.where(data_dummies_test == np.nan, data_dummies_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединяем числовые признаки и закодированные категориальные\n",
    "test_data = np.hstack((data_numb_test.values, data_dummies_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 111)\n"
     ]
    }
   ],
   "source": [
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dummies_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = clf.predict(test_data)\n",
    "test_labels_proba = clf.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраняем результат для Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_labels_proba[:,1], columns=['result'])  \n",
    "df.index.name = 'ID'\n",
    "# df['index'] = df.index\n",
    "# df.astype({\"ID\": int, \"result\": float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('output.csv', df, delimiter=',', fmt='%f', header='ID, result')\n",
    "df.to_csv('output.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
